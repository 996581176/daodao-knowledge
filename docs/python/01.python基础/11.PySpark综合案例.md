---
title: PySpark综合案例
date: 2024-04-01 11:28:35
permalink: /pages/b20141/
categories:
  - python
  - python基础
tags:
    -
---
## Spark是什么
定义：Apache Spark是用于大规模数据(large-scala data) 处理的统一(unified)分析引擎。  
简单来说，Spark是一款分布式的计算框架，用于调度成百上千的服务器集群，计算TB、PB乃至EB级别的海量数据。 
Spark作为全球顶级的分布式计算框架，支持众多的编程语言进行开发。而Python语言，则是Spark重点支持的方向。

### PySpark
Spark对Python语言的支持，重点体现在，Python第三方库：PySpark之上。  

PySpark是由Spark官方开发的Python语言第三方库。  
Python开发者可以使用pip程序快速的安装PySpark并像其它第三方库那样直接使用。  

## 基础准备
### PySpark库的安装
同其它的第三方库一样，PySpark同样可以使用pip程序进行安装。  

在“CMD”命令提示符程序内，输入：  
pip install pyspark  
或者使用国内代理镜像网站(清华大学源)  
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pyspark  

### 构建PySpark执行环境入口对象
想要使用PySpark库完成数据库处理，首先需要构建一个执行环境入口对象。  
PySpark的执行环境入口对象是：类 SparkContext 的类对象。  
```py
# 导包 
from pyspark import SparkConf, SparkContext

# 创建SparkConf对象
conf = SparkConf().setMaster("local[*]").setAppName("test_spark_app")

# 基于SparkConf类对象创建SparkContext类对象
sc = SparkContext(conf=conf)

# 打印PySpark的运行版本
print(sc.version)

# 停止SparkContext对象的运行（停止PySpark程序）
sc.stop()
```

::: warning
第一次运行报错了
![](https://daodaoblogpicgo.oss-cn-shanghai.aliyuncs.com/img/202404011326604.png)

原因是我没有安装java的jdk:
1. 安装Java：如果未安装，请前往Oracle官网下载并安装Java。
2. 设置JAVA_HOME环境变量：打开“系统属性”，选择“高级”选项卡，点击“环境变量”。在系统变量中点击“新建”，变量名输入JAVA_HOME，变量值输入Java安装目录的路径,不用带bin。
3. 在path中添加 %JAVA_HOME%\bin
4. ![](https://daodaoblogpicgo.oss-cn-shanghai.aliyuncs.com/img/202404011347486.png)
5. 然后配置JAVA_HOME
  ![](https://daodaoblogpicgo.oss-cn-shanghai.aliyuncs.com/img/202404011348290.png)

再次运行就不报错了。
:::

### PySpark的编程模型
SparkContext类对象，是PySpark编程中一切功能的入口。  
PySpark的编程，主要分为如下三大步骤：  
- 数据输入  
  通过SparkContext类对象的成员方法，完成数据的读取操作，读取后得到RDD类对象。 

- 数据处理计算  
  通过RDD类对象的成员方法，完成各种数据计算的需求。 

- 数据输出  
  将处理完成后的RDD对象，调用各种成员方法完成，写出文件、转换为list等操作。

## 数据输入
### RDD对象
PySpark支持多种数据的输入，在输入完成后，都会得到一个：RDD类的对象。  
RDD全程为：弹性分布式数据集（Resilient Distributed Datasets）  
PySpark针对数据的处理，都是以RDD对象作为载体，即：  
- 数据存储在RDD内  
- 各类数据的计算方法，也都是RDD的成员方法  
- RDD的数据计算方法，返回值依旧是RDD对象  

### Python数据容器转RDD对象
PySpark支持通过SparkContext对象的parallelize成员方法，将：  
- list  
- tuple  
- set  
- dict  
- str  
转换为PySpark的RDD对象

::: warning
- 字符串会被拆分出1个个的字符，存入RDD对象  
- 字典仅有key会被存入RDD对象  
:::

```py
from pyspark import SparkConf, SparkContext

conf = SparkConf().setMaster("local[*]").setAppName("test_spark_app")

sc = SparkContext(conf=conf)

# 通过parallelize方法将Python对象加载到Spark内，成为RDD对象
rdd1 = sc.parallelize([1,2,3,4,5])
rdd2 = sc.parallelize((1,2,3,4,5))
rdd3 = sc.parallelize("abcdefg")
rdd4 = sc.parallelize({1,2,3,4,5})
rdd5 = sc.parallelize({"key1":"value1", "key2":"value2"})

# 如果要查看RDD里面有什么内容，需要用collect()方法
print(rdd1.collect()) # [1, 2, 3, 4, 5]
print(rdd2.collect()) # [1, 2, 3, 4, 5]
print(rdd3.collect()) # ['a', 'b', 'c', 'd', 'e', 'f', 'g']
print(rdd4.collect()) # [1, 2, 3, 4, 5]
print(rdd5.collect()) # ['key1', 'key2']

sc.stop()
```

### 读取文件转RDD对象
PySpark也支持通过SparkContext入口对象，来读取文件，来构建出RDD对象。  
```py
from pyspark import SparkConf, SparkContext

conf = SparkConf().setMaster("local[*]").setAppName("test_spark_app")

sc = SparkContext(conf=conf)

# 用textFile方法，读取文件数据加载到Spark内，成为RDD对象
rdd = sc.textFile("./hello.txt")
print(rdd.collect()) # ['itheima', 'nihao', 'daodao', 'youyou', 'haha']

sc.stop()
```

## 数据计算
### map方法
PySpark的数据计算，都是基于RDD对象来进行的，那么如何进行呢？  
自然是依赖RDD对象内置丰富的：成员方法（算子）  

#### map算子
功能：map算子，是将RDD的数据，一条条处理（处理的逻辑基于map算子中接收的处理函数），返回新的RDD  
语法：
```py
rdd.map(func)
# func: f:(T) -> U
# f: 表示这是一个函数（方法）
# （T) -> U 表示的是方法的定义：
#    （）表示传入参数，(T)表示传入1个参数，()表示没有传入参数
# T是泛型的代称，在这里表示 任意类型 
# U也是泛型的代称，在这里表示 任意类型
# -> U 表示返回值
# (T) -> U 总结起来的意思是：这是一个方法，这个方法接受一个参数传入，传入参数类型不限，返回一个返回值，返回值类型不限
# (A) -> A 总结起来的意思是：这事一个方法，这个方法接受一个参数传入，传入参数类型不限，返回一个返回值，返回值和传入参数类型一致。
```